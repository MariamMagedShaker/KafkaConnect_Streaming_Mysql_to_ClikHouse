# Kafka_Streaming_Mysql_to_ClikHouse
## ğŸš€ Real-Time Data Streaming Pipeline: MySQL â†’ ClickHouse Cloud

This project demonstrates a **real-time data streaming pipeline** that captures live changes from **MySQL** and streams them into **ClickHouse Cloud** for lightning-fast analytics.
It uses **Python, Debezium, Kafka, Kafka Connect, and Docker Compose** to build a scalable, containerized solution.

---

## ğŸ“Œ Project Overview

The pipeline enables **continuous data flow** from source to destination with minimal latency:

1. **Data Simulation** â€“ A Python script generates IoT-like sensor data and inserts it into **MySQL**.
2. **Change Data Capture (CDC)** â€“ **Debezium** monitors MySQL and captures all inserts/updates in real time.
3. **Streaming Transport** â€“ **Apache Kafka** handles event streaming between source and sink.
4. **Sink Integration** â€“ **Kafka Connect** with the **ClickHouse Sink Connector** writes Kafka events into **ClickHouse Cloud** tables.
5. **Containerized Setup** â€“ All components run seamlessly via **Docker Compose**.

---

## ğŸ§© Tech Stack

âœ… Python â€“ data generation & insertion into MySQL 
âœ… MySQL â€“ source database
âœ… Debezium â€“ CDC from MySQL
âœ… Apache Kafka â€“ event streaming platform
âœ… Kafka Connect â€“ integration framework
âœ… ClickHouse Sink Connector â€“ loads data into ClickHouse Cloud
âœ… ClickHouse Cloud â€“ destination for real-time analytics
âœ… Docker Compose â€“ container orchestration

---

## âš™ï¸ Pipeline Workflow

```
Python â†’ MySQL â†’ Debezium (CDC) â†’ Kafka â†’ Kafka Connect â†’ ClickHouse Cloud
```

---

## ğŸ“‚ Repository Structure

```
â”œâ”€â”€ docker-compose.yml        # Orchestrates MySQL, Kafka, Debezium, Kafka Connect
â”œâ”€â”€ sensors.sql               # MySQL table schema
â”œâ”€â”€ data_producer.py          # Python script to generate & insert sensor data
â”œâ”€â”€ connect-config.json       # Kafka Connect ClickHouse sink configuration
â””â”€â”€ README.md                 # Project documentation
```
---
